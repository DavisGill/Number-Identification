{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wlw3GQlWH3i"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZeXQs9dWvgR",
        "outputId": "fa2cd9a0-a888-4abf-c37a-db305dbeaea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Data Preparation\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DG3FPjFRY5Od"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU--N5lWWxUI",
        "outputId": "a4d6d419-f129-4ddd-8c0f-2e1d1380df56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1500/1500 [==============================] - 23s 9ms/step - loss: 0.2140 - accuracy: 0.9346 - val_loss: 0.0527 - val_accuracy: 0.9834\n",
            "Epoch 2/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0770 - accuracy: 0.9762 - val_loss: 0.0409 - val_accuracy: 0.9883\n",
            "Epoch 3/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0632 - accuracy: 0.9811 - val_loss: 0.0369 - val_accuracy: 0.9889\n",
            "Epoch 4/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0508 - accuracy: 0.9845 - val_loss: 0.0319 - val_accuracy: 0.9908\n",
            "Epoch 5/10\n",
            "1500/1500 [==============================] - 13s 8ms/step - loss: 0.0462 - accuracy: 0.9862 - val_loss: 0.0299 - val_accuracy: 0.9923\n",
            "Epoch 6/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0406 - accuracy: 0.9876 - val_loss: 0.0270 - val_accuracy: 0.9929\n",
            "Epoch 7/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0365 - accuracy: 0.9888 - val_loss: 0.0353 - val_accuracy: 0.9905\n",
            "Epoch 8/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0346 - accuracy: 0.9894 - val_loss: 0.0292 - val_accuracy: 0.9912\n",
            "Epoch 9/10\n",
            "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0296 - accuracy: 0.9910 - val_loss: 0.0260 - val_accuracy: 0.9932\n",
            "Epoch 10/10\n",
            "1500/1500 [==============================] - 13s 9ms/step - loss: 0.0259 - accuracy: 0.9922 - val_loss: 0.0254 - val_accuracy: 0.9930\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Training and Validation\n",
        "history = model.fit(train_images, train_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC9qgUtwDKQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "079c9d73-2ce5-4a6e-bb7e-f2d661efc4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model.save(\"validation_cnn.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76JHRtv7YJbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd77876-c636-4e87-b6ba-52036a7a1818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 26, 26, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 24, 24, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 32)        25632     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 12, 12, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 10, 10, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 4, 4, 64)          102464    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 4, 4, 64)          256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1024)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               131200    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 327242 (1.25 MB)\n",
            "Trainable params: 326410 (1.25 MB)\n",
            "Non-trainable params: 832 (3.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-djk9vnBWysb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7869f812-5583-4085-8f93-aff8324631b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy greater than 99% achieved.\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Model Evaluation\n",
        "val_acc = history.history['val_accuracy'][-1]\n",
        "if val_acc > 0.99:\n",
        "    print(\"Validation accuracy greater than 99% achieved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQY75BxTXinR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d2b8d8-7b0a-43be-fe86-d727e3343e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9929999709129333\n"
          ]
        }
      ],
      "source": [
        "print(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqa4anTEW0mQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18fd50f-b83c-455f-cf4c-631ddf8ad60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 13s 15ms/step - loss: 0.2451 - accuracy: 0.9254\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0668 - accuracy: 0.9796\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0497 - accuracy: 0.9845\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0416 - accuracy: 0.9872\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0348 - accuracy: 0.9893\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 7s 15ms/step - loss: 0.0331 - accuracy: 0.9897\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0295 - accuracy: 0.9906\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0271 - accuracy: 0.9915\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0254 - accuracy: 0.9921\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 7s 14ms/step - loss: 0.0247 - accuracy: 0.9922\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Define the model architecture\n",
        "final_model = Sequential()\n",
        "\n",
        "final_model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Dropout(0.4))\n",
        "\n",
        "final_model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Dropout(0.4))\n",
        "\n",
        "final_model.add(Flatten())\n",
        "final_model.add(Dense(128, activation='relu'))\n",
        "final_model.add(BatchNormalization())\n",
        "final_model.add(Dropout(0.4))\n",
        "final_model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "final_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the final_model on the entire training set\n",
        "history = final_model.fit(train_images, train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRhXMj22DFkx"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "final_model.save(\"final_cnn.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYApotuhXd2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2289fb-a397-47da-a2ae-a6750588ee5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 26, 26, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 24, 24, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 12, 12, 32)        25632     \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 12, 12, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 10, 10, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 10, 10, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_10 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 8, 8, 64)          256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " conv2d_11 (Conv2D)          (None, 4, 4, 64)          102464    \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 4, 4, 64)          256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 4, 4, 64)          0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               131200    \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 128)               512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 327242 (1.25 MB)\n",
            "Trainable params: 326410 (1.25 MB)\n",
            "Non-trainable params: 832 (3.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "final_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plGLId9dW2kw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6ae76e05-2c00-48f9-af1c-e0bc7044919e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-93ea13716ac7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the final_model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model' is not defined"
          ]
        }
      ],
      "source": [
        "# Evaluate the final_model on the test set\n",
        "test_loss, test_acc = final_model.evaluate(test_images, test_labels)\n",
        "print(\"Test accuracy:\", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8epJ4m5W3-h"
      },
      "outputs": [],
      "source": [
        "# Step 7: Misclassification Analysis\n",
        "predictions = final_model.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(test_labels, axis=1)\n",
        "misclassified_indices = np.where(predicted_classes != true_classes)[0]\n",
        "print(\"Number of misclassified samples:\", len(misclassified_indices))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYeN5adCW5RZ"
      },
      "outputs": [],
      "source": [
        "# Plot some misclassified samples\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, idx in enumerate(misclassified_indices[:5]):\n",
        "    plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(test_images[idx].reshape(28, 28), cmap='gray')\n",
        "    predicted_label = np.argmax(predictions[idx])\n",
        "    true_label = np.argmax(test_labels[idx])\n",
        "    plt.title(f\"Predicted: {predicted_label}\\nTrue: {true_label}\", fontsize=10, pad=5)\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0nd0DNo9Cpb"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IxMJlj5W66r"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjnfIee29EyU"
      },
      "outputs": [],
      "source": [
        "# this is written as a tensorflow \"layer\".  it's just a vector the same size as the\n",
        "# output of the previous layer. the vector is initialized randomly, but we'll use\n",
        "# gradient descent to update the values in the vector\n",
        "#\n",
        "# it's purpose is to be appended to the beginning of the sequence of vectors fed into\n",
        "# the transformer.  then after the transformer runs on the whole data, we just grab\n",
        "# the resulting zero-th vector...the class token...and use that as the portfolio weights\n",
        "class ClassToken(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1URAlHB9JQo"
      },
      "outputs": [],
      "source": [
        "def build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes):\n",
        "    # n is number of rows of blocks\n",
        "    # m is number of cols of blocks\n",
        "    # block_size is number of pixels (with rgb) in each block\n",
        "\n",
        "    inp = tf.keras.layers.Input(shape=(n*m,block_size))\n",
        "    inp2 = tf.keras.layers.Input(shape=(n*m))\n",
        "    mid = tf.keras.layers.Dense(hidden_dim)(inp) # transform to vectors with different dimension\n",
        "    # the positional embeddings\n",
        "#     positions = tf.range(start=0, limit=n*m, delta=1)\n",
        "    emb = tf.keras.layers.Embedding(input_dim=n*m, output_dim=hidden_dim)(inp2) # learned positional embedding for each of the n*m possible possitions\n",
        "    mid = mid + emb # for some reason, tf.keras.layers.Add causes an error, but + doesn't?\n",
        "    # create and append class token to beginning of all input vectors\n",
        "    token = ClassToken()(mid) # append class token to beginning of sequence\n",
        "    mid = tf.keras.layers.Concatenate(axis=1)([token, mid])\n",
        "\n",
        "    for l in range(num_layers): # how many Transformer Head layers are there?\n",
        "        ln  = tf.keras.layers.LayerNormalization()(mid) # normalize\n",
        "        mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads,key_dim=key_dim,value_dim=key_dim)(ln,ln,ln) # self attention!\n",
        "        add = tf.keras.layers.Add()([mid,mha]) # add and norm\n",
        "        ln  = tf.keras.layers.LayerNormalization()(add)\n",
        "        den = tf.keras.layers.Dense(mlp_dim,activation='gelu')(ln) # maybe should be relu...who knows...\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den) # regularization\n",
        "        den = tf.keras.layers.Dense(hidden_dim)(den) # back to the right dimensional space\n",
        "        den = tf.keras.layers.Dropout(dropout_rate)(den)\n",
        "        mid = tf.keras.layers.Add()([den,add]) # add and norm again\n",
        "    ln = tf.keras.layers.LayerNormalization()(mid)\n",
        "    fl = ln[:,0,:] # just grab the class token for each image in batch\n",
        "    clas = tf.keras.layers.Dense(num_classes,activation='softmax')(fl) # probability that the image is in each category\n",
        "    mod = tf.keras.models.Model([inp,inp2],clas)\n",
        "    mod.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "    return mod"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qyVFYUS9KTg"
      },
      "outputs": [],
      "source": [
        "n = 4\n",
        "m = 4\n",
        "block_size = 49\n",
        "hidden_dim = 128\n",
        "num_layers = 8\n",
        "num_heads = 8\n",
        "key_dim = hidden_dim//num_heads # usually good practice for key_dim to be hidden_dim//num_heads...this is why we do Multi-Head attention\n",
        "mlp_dim = hidden_dim\n",
        "dropout_rate = 0.13\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "\n",
        "trans = build_ViT(n,m,block_size,hidden_dim,num_layers,num_heads,key_dim,mlp_dim,dropout_rate,num_classes)\n",
        "trans.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqzHa2639LWr"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "ndata_train = x_train.shape[0]\n",
        "ndata_test = x_test.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmhEG-bq9MSB"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnMKnbeM31RX"
      },
      "outputs": [],
      "source": [
        "x_train_ravel = np.zeros((ndata_train, n * m, block_size))\n",
        "for img in range(ndata_train):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            block = x_train[img, row * int(np.sqrt(block_size)):(row + 1) * int(np.sqrt(block_size)), col * int(np.sqrt(block_size)):(col + 1) * int(np.sqrt(block_size))].reshape(-1)\n",
        "            x_train_ravel[img, ind, :] = block\n",
        "            ind += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlQmTqa69OqY"
      },
      "outputs": [],
      "source": [
        "x_test_ravel = np.zeros((ndata_test, n * m, block_size))\n",
        "for img in range(ndata_test):\n",
        "    ind = 0\n",
        "    for row in range(n):\n",
        "        for col in range(m):\n",
        "            block = x_test[img, row * int(np.sqrt(block_size)):(row + 1) * int(np.sqrt(block_size)), col * int(np.sqrt(block_size)):(col + 1) * int(np.sqrt(block_size))].reshape(-1)\n",
        "            x_test_ravel[img, ind, :] = block\n",
        "            ind += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xTcLPsw9PeN"
      },
      "outputs": [],
      "source": [
        "pos_feed_train = np.array([list(range(n*m))]*ndata_train)\n",
        "pos_feed_test = np.array([list(range(n*m))]*ndata_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VA9aFD6caygI",
        "outputId": "2c5a9e16-489e-49ab-8484-62d84e0615c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "300/300 [==============================] - 36s 43ms/step - loss: 0.6665 - accuracy: 0.7774 - val_loss: 0.1888 - val_accuracy: 0.9442 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "300/300 [==============================] - 12s 39ms/step - loss: 0.1635 - accuracy: 0.9481 - val_loss: 0.1278 - val_accuracy: 0.9640 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "300/300 [==============================] - 12s 38ms/step - loss: 0.1169 - accuracy: 0.9631 - val_loss: 0.1052 - val_accuracy: 0.9680 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0948 - accuracy: 0.9697 - val_loss: 0.1061 - val_accuracy: 0.9697 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0775 - accuracy: 0.9748 - val_loss: 0.0964 - val_accuracy: 0.9721 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "300/300 [==============================] - 12s 38ms/step - loss: 0.0696 - accuracy: 0.9780 - val_loss: 0.0878 - val_accuracy: 0.9741 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 0.0667 - accuracy: 0.9778 - val_loss: 0.0791 - val_accuracy: 0.9765 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0613 - accuracy: 0.9800 - val_loss: 0.0712 - val_accuracy: 0.9797 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0526 - accuracy: 0.9827 - val_loss: 0.0894 - val_accuracy: 0.9745 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0534 - accuracy: 0.9826 - val_loss: 0.0709 - val_accuracy: 0.9792 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0430 - accuracy: 0.9858 - val_loss: 0.0658 - val_accuracy: 0.9814 - lr: 9.0484e-04\n",
            "Epoch 12/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0384 - accuracy: 0.9870 - val_loss: 0.0724 - val_accuracy: 0.9796 - lr: 8.1873e-04\n",
            "Epoch 13/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.0670 - val_accuracy: 0.9812 - lr: 7.4082e-04\n",
            "Epoch 14/50\n",
            "300/300 [==============================] - 12s 39ms/step - loss: 0.0221 - accuracy: 0.9924 - val_loss: 0.0587 - val_accuracy: 0.9842 - lr: 6.7032e-04\n",
            "Epoch 15/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 0.0217 - accuracy: 0.9925 - val_loss: 0.0580 - val_accuracy: 0.9843 - lr: 6.0653e-04\n",
            "Epoch 16/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0176 - accuracy: 0.9939 - val_loss: 0.0615 - val_accuracy: 0.9842 - lr: 5.4881e-04\n",
            "Epoch 17/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 0.0121 - accuracy: 0.9961 - val_loss: 0.0707 - val_accuracy: 0.9827 - lr: 4.9659e-04\n",
            "Epoch 18/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0123 - accuracy: 0.9961 - val_loss: 0.0643 - val_accuracy: 0.9842 - lr: 4.4933e-04\n",
            "Epoch 19/50\n",
            "300/300 [==============================] - 11s 35ms/step - loss: 0.0110 - accuracy: 0.9964 - val_loss: 0.0637 - val_accuracy: 0.9833 - lr: 4.0657e-04\n",
            "Epoch 20/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0646 - val_accuracy: 0.9849 - lr: 3.6788e-04\n",
            "Epoch 21/50\n",
            "300/300 [==============================] - 11s 35ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0705 - val_accuracy: 0.9828 - lr: 3.3287e-04\n",
            "Epoch 22/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0668 - val_accuracy: 0.9858 - lr: 3.0119e-04\n",
            "Epoch 23/50\n",
            "300/300 [==============================] - 10s 35ms/step - loss: 0.0051 - accuracy: 0.9984 - val_loss: 0.0672 - val_accuracy: 0.9844 - lr: 2.7253e-04\n",
            "Epoch 24/50\n",
            "300/300 [==============================] - 10s 34ms/step - loss: 0.0041 - accuracy: 0.9987 - val_loss: 0.0615 - val_accuracy: 0.9864 - lr: 2.4660e-04\n",
            "Epoch 25/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0641 - val_accuracy: 0.9855 - lr: 2.2313e-04\n",
            "Epoch 26/50\n",
            "300/300 [==============================] - 11s 35ms/step - loss: 0.0030 - accuracy: 0.9990 - val_loss: 0.0624 - val_accuracy: 0.9870 - lr: 2.0190e-04\n",
            "Epoch 27/50\n",
            "300/300 [==============================] - 11s 37ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0615 - val_accuracy: 0.9863 - lr: 1.8268e-04\n",
            "Epoch 28/50\n",
            "300/300 [==============================] - 10s 35ms/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0603 - val_accuracy: 0.9861 - lr: 1.6530e-04\n",
            "Epoch 29/50\n",
            "300/300 [==============================] - 11s 35ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0644 - val_accuracy: 0.9861 - lr: 1.4957e-04\n",
            "Epoch 30/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0640 - val_accuracy: 0.9872 - lr: 1.3534e-04\n",
            "Epoch 31/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0623 - val_accuracy: 0.9877 - lr: 1.2246e-04\n",
            "Epoch 32/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 7.0385e-04 - accuracy: 0.9999 - val_loss: 0.0611 - val_accuracy: 0.9874 - lr: 1.1080e-04\n",
            "Epoch 33/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 9.0104e-04 - accuracy: 0.9998 - val_loss: 0.0629 - val_accuracy: 0.9872 - lr: 1.0026e-04\n",
            "Epoch 34/50\n",
            "300/300 [==============================] - 11s 35ms/step - loss: 8.0472e-04 - accuracy: 0.9998 - val_loss: 0.0659 - val_accuracy: 0.9876 - lr: 9.0718e-05\n",
            "Epoch 35/50\n",
            "300/300 [==============================] - 10s 34ms/step - loss: 6.2728e-04 - accuracy: 0.9999 - val_loss: 0.0646 - val_accuracy: 0.9879 - lr: 8.2085e-05\n",
            "Epoch 36/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 6.5577e-04 - accuracy: 0.9998 - val_loss: 0.0628 - val_accuracy: 0.9883 - lr: 7.4273e-05\n",
            "Epoch 37/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 5.9690e-04 - accuracy: 0.9999 - val_loss: 0.0651 - val_accuracy: 0.9871 - lr: 6.7205e-05\n",
            "Epoch 38/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 5.5919e-04 - accuracy: 0.9999 - val_loss: 0.0639 - val_accuracy: 0.9883 - lr: 6.0810e-05\n",
            "Epoch 39/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 6.3354e-04 - accuracy: 0.9998 - val_loss: 0.0642 - val_accuracy: 0.9872 - lr: 5.5023e-05\n",
            "Epoch 40/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 4.3391e-04 - accuracy: 0.9999 - val_loss: 0.0658 - val_accuracy: 0.9877 - lr: 4.9787e-05\n",
            "Epoch 41/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 4.9474e-04 - accuracy: 0.9999 - val_loss: 0.0638 - val_accuracy: 0.9881 - lr: 4.5049e-05\n",
            "Epoch 42/50\n",
            "300/300 [==============================] - 12s 38ms/step - loss: 3.2534e-04 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9877 - lr: 4.0762e-05\n",
            "Epoch 43/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 3.0475e-04 - accuracy: 0.9999 - val_loss: 0.0671 - val_accuracy: 0.9876 - lr: 3.6883e-05\n",
            "Epoch 44/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 3.0090e-04 - accuracy: 1.0000 - val_loss: 0.0657 - val_accuracy: 0.9883 - lr: 3.3373e-05\n",
            "Epoch 45/50\n",
            "300/300 [==============================] - 11s 38ms/step - loss: 3.0022e-04 - accuracy: 0.9999 - val_loss: 0.0654 - val_accuracy: 0.9877 - lr: 3.0197e-05\n",
            "Epoch 46/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 3.2743e-04 - accuracy: 0.9999 - val_loss: 0.0666 - val_accuracy: 0.9876 - lr: 2.7324e-05\n",
            "Epoch 47/50\n",
            "300/300 [==============================] - 12s 38ms/step - loss: 2.1008e-04 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9876 - lr: 2.4723e-05\n",
            "Epoch 48/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 3.4003e-04 - accuracy: 0.9999 - val_loss: 0.0640 - val_accuracy: 0.9877 - lr: 2.2371e-05\n",
            "Epoch 49/50\n",
            "300/300 [==============================] - 11s 36ms/step - loss: 2.0671e-04 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9882 - lr: 2.0242e-05\n",
            "Epoch 50/50\n",
            "300/300 [==============================] - 10s 34ms/step - loss: 2.2444e-04 - accuracy: 0.9999 - val_loss: 0.0644 - val_accuracy: 0.9879 - lr: 1.8316e-05\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d271a7c6200>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define your learning rate scheduler function\n",
        "def lr_scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr  # Keep the initial learning rate for the first 10 epochs\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)  # Exponentially decay the learning rate after 10 epochs\n",
        "\n",
        "# Create the learning rate scheduler callback\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
        "\n",
        "# Define the model checkpoint callback to save the best model weights\n",
        "checkpoint_filepath = 'model_checkpoint_v2.h5'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Call trans.fit() with the learning rate scheduler and model checkpoint callbacks\n",
        "trans.fit([x_train_ravel, pos_feed_train], y_train,\n",
        "          epochs=50, batch_size=160, validation_split=0.20,\n",
        "          callbacks=[lr_callback, model_checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b8UhYxtF9TXs"
      },
      "outputs": [],
      "source": [
        "# Create a new instance of your model\n",
        "trans = build_ViT(n, m, block_size, hidden_dim, num_layers, num_heads, key_dim, mlp_dim, dropout_rate, num_classes)\n",
        "\n",
        "# Load the weights from the checkpoint file\n",
        "checkpoint_filepath = 'model_checkpoint_v2.h5'\n",
        "trans.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Compile the model (if necessary)\n",
        "# trans.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ql_wgECFiSRz",
        "outputId": "0a3f1ea5-e495-4a39-e73f-6ac7d5c130c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 9s 21ms/step - loss: 0.0526 - accuracy: 0.9897\n"
          ]
        }
      ],
      "source": [
        "out = trans.evaluate([x_test_ravel,pos_feed_test],y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i5yxOYySCLPJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anvil Uplink"
      ],
      "metadata": {
        "id": "Sntcyo50zoM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install anvil-uplink"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "id": "dfc6MIwFzqSU",
        "outputId": "5a846058-5ab3-4b8a-d707-337d78c085ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.4.2-py2.py3-none-any.whl (90 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/90.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Collecting ws4py (from anvil-uplink)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=e2c0f8ff6c39899ca95ed14c4cf92df2b16fb2b517c6140abe460721cb5e7e0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.4.2 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import anvil.server\n",
        "\n",
        "anvil.server.connect(\"server_X5QT7YAEYYWLOO7WLNIVSWMW-OZ5H5BS2NZJKEVCA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q06A_SEzzxxj",
        "outputId": "0ac13a34-60ce-4392-e410-400a5f9a9ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Development\" as SERVER\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}